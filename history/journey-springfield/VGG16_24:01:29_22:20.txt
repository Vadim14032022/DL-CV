----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 100, 100]           1,792
              ReLU-2         [-1, 64, 100, 100]               0
            Conv2d-3         [-1, 64, 100, 100]          36,928
              ReLU-4         [-1, 64, 100, 100]               0
         MaxPool2d-5           [-1, 64, 50, 50]               0
            Conv2d-6          [-1, 128, 50, 50]          73,856
              ReLU-7          [-1, 128, 50, 50]               0
            Conv2d-8          [-1, 128, 50, 50]         147,584
              ReLU-9          [-1, 128, 50, 50]               0
        MaxPool2d-10          [-1, 128, 25, 25]               0
           Conv2d-11          [-1, 256, 25, 25]         295,168
             ReLU-12          [-1, 256, 25, 25]               0
           Conv2d-13          [-1, 256, 25, 25]         590,080
             ReLU-14          [-1, 256, 25, 25]               0
           Conv2d-15          [-1, 256, 25, 25]         590,080
             ReLU-16          [-1, 256, 25, 25]               0
        MaxPool2d-17          [-1, 256, 12, 12]               0
           Conv2d-18          [-1, 512, 12, 12]       1,180,160
             ReLU-19          [-1, 512, 12, 12]               0
           Conv2d-20          [-1, 512, 12, 12]       2,359,808
             ReLU-21          [-1, 512, 12, 12]               0
           Conv2d-22          [-1, 512, 12, 12]       2,359,808
             ReLU-23          [-1, 512, 12, 12]               0
        MaxPool2d-24            [-1, 512, 6, 6]               0
           Conv2d-25            [-1, 512, 6, 6]       2,359,808
             ReLU-26            [-1, 512, 6, 6]               0
           Conv2d-27            [-1, 512, 6, 6]       2,359,808
             ReLU-28            [-1, 512, 6, 6]               0
           Conv2d-29            [-1, 512, 6, 6]       2,359,808
             ReLU-30            [-1, 512, 6, 6]               0
        MaxPool2d-31            [-1, 512, 3, 3]               0
AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0
      BatchNorm1d-33                [-1, 25088]          50,176
           Linear-34                   [-1, 42]       1,053,738
              VGG-35                   [-1, 42]               0
================================================================
Total params: 15,818,602
Trainable params: 8,183,338
Non-trainable params: 7,635,264
----------------------------------------------------------------
Input size (MB): 0.11
Forward/backward pass size (MB): 43.51
Params size (MB): 60.34
Estimated Total Size (MB): 103.97
----------------------------------------------------------------
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): BatchNorm1d(25088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Linear(in_features=25088, out_features=42, bias=True)
  )
)
Epoch  train_loss  train_acc  val_loss  val_acc
    1    1.465883   0.675943  2.516835 0.639657
    2    0.902738   0.801680  3.141571 0.709485
    3    0.763708   0.838041  3.160696 0.765435
    4    0.635201   0.848767  3.392739 0.757569
    5    0.492363   0.873853  3.021325 0.842670
    6    0.354207   0.903641  2.878311 0.819113
    7    0.277876   0.923306  1.517637 0.892993
    8    0.220120   0.937671  1.587022 0.850536
    9    0.356659   0.908831  1.205530 0.878188
   10    0.318578   0.914909  1.732743 0.888200
   11    0.153918   0.954830  1.326548 0.931363
   12    0.095237   0.971337  1.384039 0.940658
   13    0.058486   0.981230  1.651142 0.943266
   14    0.048514   0.984448  1.397817 0.958045
   15    0.037159   0.987844  2.062321 0.959237
   16    0.030259   0.990644  1.403824 0.958055
   17    0.023797   0.992670  0.919945 0.967827
   18    0.017069   0.994995  1.032377 0.967580
   19    0.019009   0.995054  1.502101 0.968534
   20    0.014168   0.996365  0.798407 0.969487
   21    0.011053   0.997318  1.004478 0.964013
   22    0.011401   0.996544  1.042455 0.974261
   23    0.009608   0.997616  0.915036 0.972110
   24    0.009075   0.998153  1.395883 0.967104
   25    0.011102   0.997319  0.799250 0.967580
   26    0.009448   0.997080  1.700405 0.964013
   27    0.007820   0.998331  0.911376 0.967588
   28    0.007998   0.998212  0.868473 0.971156
   29    0.008610   0.997914  1.212722 0.967819
   30    0.009111   0.997438  0.990167 0.970203
   31    0.007439   0.998093  0.728142 0.969971
   32    0.007555   0.998033  0.889688 0.973308
   33    0.008450   0.997974  1.419532 0.972110
   34    0.007167   0.998808  1.628849 0.968772
   35    0.006381   0.998629  0.754190 0.972825
   36    0.185984   0.951076  1.713937 0.876311
   37    0.257805   0.936237  0.826235 0.921115
   38    0.190120   0.950423  1.560667 0.911800
   39    0.190144   0.950185  1.949434 0.939213
   40    0.107268   0.969968  1.528515 0.933254
   41    0.103784   0.971694  1.290400 0.953527
   42    0.109878   0.969370  2.022766 0.938751
   43    0.096131   0.975212  3.009849 0.940644
   44    0.082802   0.979561  0.920584 0.966389
   45    0.136999   0.967942  1.536455 0.945411
   46    0.048568   0.985996  1.733182 0.961392
   47    0.022660   0.993087  1.047359 0.973546
   48    0.013538   0.995710  1.884707 0.966389
   49    0.008013   0.997319  1.527441 0.975209
   50    0.006694   0.997557  1.150788 0.978546
   51    0.007320   0.997616  1.470754 0.972593
   52    0.005051   0.998331  1.110027 0.980696
   53    0.005826   0.998212  1.044586 0.980215
   54    0.004688   0.998689  1.001503 0.976162
   55    0.005986   0.998034  1.174312 0.975209
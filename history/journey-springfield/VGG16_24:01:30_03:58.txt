----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1         [-1, 64, 100, 100]           1,792
              ReLU-2         [-1, 64, 100, 100]               0
            Conv2d-3         [-1, 64, 100, 100]          36,928
              ReLU-4         [-1, 64, 100, 100]               0
         MaxPool2d-5           [-1, 64, 50, 50]               0
            Conv2d-6          [-1, 128, 50, 50]          73,856
              ReLU-7          [-1, 128, 50, 50]               0
            Conv2d-8          [-1, 128, 50, 50]         147,584
              ReLU-9          [-1, 128, 50, 50]               0
        MaxPool2d-10          [-1, 128, 25, 25]               0
           Conv2d-11          [-1, 256, 25, 25]         295,168
             ReLU-12          [-1, 256, 25, 25]               0
           Conv2d-13          [-1, 256, 25, 25]         590,080
             ReLU-14          [-1, 256, 25, 25]               0
           Conv2d-15          [-1, 256, 25, 25]         590,080
             ReLU-16          [-1, 256, 25, 25]               0
        MaxPool2d-17          [-1, 256, 12, 12]               0
           Conv2d-18          [-1, 512, 12, 12]       1,180,160
             ReLU-19          [-1, 512, 12, 12]               0
           Conv2d-20          [-1, 512, 12, 12]       2,359,808
             ReLU-21          [-1, 512, 12, 12]               0
           Conv2d-22          [-1, 512, 12, 12]       2,359,808
             ReLU-23          [-1, 512, 12, 12]               0
        MaxPool2d-24            [-1, 512, 6, 6]               0
           Conv2d-25            [-1, 512, 6, 6]       2,359,808
             ReLU-26            [-1, 512, 6, 6]               0
           Conv2d-27            [-1, 512, 6, 6]       2,359,808
             ReLU-28            [-1, 512, 6, 6]               0
           Conv2d-29            [-1, 512, 6, 6]       2,359,808
             ReLU-30            [-1, 512, 6, 6]               0
        MaxPool2d-31            [-1, 512, 3, 3]               0
AdaptiveAvgPool2d-32            [-1, 512, 7, 7]               0
      BatchNorm1d-33                [-1, 25088]          50,176
           Linear-34                   [-1, 42]       1,053,738
              VGG-35                   [-1, 42]               0
================================================================
Total params: 15,818,602
Trainable params: 8,183,338
Non-trainable params: 7,635,264
----------------------------------------------------------------
Input size (MB): 0.11
Forward/backward pass size (MB): 43.51
Params size (MB): 60.34
Estimated Total Size (MB): 103.97
----------------------------------------------------------------
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): BatchNorm1d(25088, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): Linear(in_features=25088, out_features=42, bias=True)
  )
)
Epoch  train_loss  train_acc  val_loss  val_acc
    1    1.361435   0.697694  2.583013 0.696378
    2    0.635148   0.837733  2.321730 0.791945
    3    0.475997   0.881599  2.012165 0.814779
    4    0.402416   0.901680  1.044745 0.844577
    5    0.368739   0.908771  3.568667 0.780930
    6    0.363109   0.917645  1.953147 0.861296
    7    0.314429   0.932185  2.076126 0.869876
    8    0.263339   0.940889  1.299028 0.901073
    9    0.188064   0.955369  1.055797 0.932539
   10    0.187008   0.955309  1.033563 0.937068
   11    0.159098   0.963053  1.607846 0.908961
   12    0.197399   0.959359  1.012697 0.921115
   13    0.204034   0.956382  0.952902 0.945411
   14    0.140465   0.973007  1.862034 0.908224
   15    0.153418   0.965439  1.457900 0.948033
   16    0.074971   0.982540  1.439841 0.960677
   17    0.035107   0.990644  1.057157 0.971163
   18    0.021325   0.993803  2.184715 0.963766
   19    0.013139   0.996306  1.338611 0.969011
   20    0.014065   0.994697  0.798398 0.974493
   21    0.010727   0.996901  1.211907 0.970925
   22    0.008877   0.997199  1.531041 0.964490
   23    0.007625   0.997616  1.663769 0.967819
   24    0.005163   0.998451  1.217311 0.966150
   25    0.005349   0.998153  1.203424 0.972586
   26    0.006208   0.998451  1.122864 0.971878
   27    0.005938   0.998629  0.954283 0.975929
   28    0.004076   0.998629  1.529789 0.974493
   29    0.004985   0.998629  1.288593 0.972110
   30    0.003939   0.998927  1.287374 0.972110
   31    0.005236   0.998570  1.069313 0.974738
   32    0.003775   0.998689  1.268761 0.976406
   33    0.003969   0.998570  0.845396 0.974732
   34    0.002131   0.999225  1.560814 0.968057
   35    0.002947   0.999166  1.078782 0.977116
   36    0.002635   0.998987  1.880336 0.967827
   37    0.001978   0.999285  1.192108 0.971163
   38    0.002325   0.999225  0.877200 0.975685
   39    0.002278   0.999404  0.988649 0.975209
   40    0.002242   0.999345  1.351492 0.971871
   41    0.002506   0.999225  1.850289 0.969018
   42    0.002182   0.999642  1.284456 0.974976
   43    0.001328   0.999464  1.149808 0.974970
   44    0.001362   0.999464  1.313809 0.972110
   45    0.001499   0.999523  0.935075 0.976400
   46    0.001084   0.999642  1.341880 0.971878
   47    0.002360   0.999464  1.096611 0.975214
   48    0.000975   0.999821  1.084965 0.978784
   49    0.001851   0.999523  1.413727 0.974970
   50    0.000975   0.999642  1.171664 0.974017
   51    0.001993   0.999225  1.141540 0.977598
   52    0.001236   0.999702  2.388180 0.969018
   53    0.001458   0.999464  0.991753 0.971871
   54    0.001594   0.999285  1.967969 0.971156
   55    0.000667   0.999881  0.843692 0.976639
   56    0.002193   0.999404  1.124526 0.974500
   57    0.002201   0.999344  1.305182 0.975929
   58    0.002015   0.999345  1.195122 0.974493
   59    0.002119   0.999523  1.031817 0.975924
   60    0.001767   0.999523  0.819271 0.974255
   61    0.000828   0.999821  1.139327 0.979981
   62    0.001579   0.999404  0.899999 0.975691
   63    0.001698   0.999345  0.881054 0.974017
   64    0.001269   0.999464  1.487599 0.976162
   65    0.001535   0.999345  1.484736 0.972110